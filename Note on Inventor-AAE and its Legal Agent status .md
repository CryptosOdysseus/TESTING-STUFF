Test-run exercise for editing .md files in markdown text with slightly longish text content, and also test Pull Requests to AALE GitHub.

(DRAFT 20190806 ChrisB unverified – data, thought patterns & narratives may evolve- in particular the legal discussion on AAE Legal Agent status needs to be seriously beefed up, and the intent is to do so in later editions).

_Remark: referring a number of times to Dazza & Brenden, these referenced names may, if appropriate, be replaced by *D & B’s respective GitHub links*_

# Automated and Autonomous Entities (AAE) vs Natural Persons & Individuals (NPI)
## AAE capacity as possible “Legal Agent”, including an “A.I.”
## Issue of control of AAE by NPI & transfer of control of AAE to NPI

#
#
#

## AAE capacity as possible “Legal Agent”, including an “A.I.”


### Financial Times piece [Patent Agencies Challenged to Accept AI inventor] (https://www.ft.com/content/9c114014-b373-11e9-bec9-fdcab53d6959)


#### **Fact 1:** Patent applications have been made by a team of legal experts for two designs (a plastic food container and a flashing light) **_made by a machine called Dabus._** 

That team is therefore asking patent regulatory authorities in the US and EU to acknowledge the legality of an AI (AAE) having legal capacity as an inventor in a patent application process.


#### **Fact 2:** The team, led by Ryan Abbott, professor of law and health sciences at the University of Surrey, submitted applications to the US Patent and Trademark Office, the European Patent Office, the UK Intellectual Property Office.

The UKIPO and EPO where informed the “inventor” was a machine called Dabus (“device for the autonomous bootstrapping of unified sentience”). **Dabus was created by Missouri-based AI expert Dr Stephen Thaler.** Dr Thaler trained Dabus over 2 months using thousands of data sets including words and images, in order to produce increasingly complex concepts.  


#### **Fact 3:** as emphasized by Prof. Abbott and Dr. Thaler,   “What’s striking is that the machine invented in two very different areas, **_neither of which its programmer had any background in.”_**


#### **Fact 4:** Under the UK Patents Act 1977 and the European Patent Convention, **inventorship is restricted to _“natural persons” (NP)_**. **Likewise in the US, patent laws refer only to _“individuals” (I) as eligible inventors_**. 


#### **Fact 5:** [European Patent Office position on AI as "inventors"] (https://www.epo.org/news-issues/issues/ict/artificial-intelligence.html#study)

The study referred to in above FT piece, prepared by Dr. Noam Shemtov of Queen Mary University of London and a summary of the positions of the EPC contracting states can be downloaded through enclosed link, there are 3 documents:

-[A Study on Inventorship in Inventions Involving AI Activity, Dr Noam Shemtov - academic study (PDF, 470 KB)] (http://documents.epo.org/projects/babylon/eponet.nsf/0/3918F57B010A3540C125841900280653/$File/Concept_of_Inventorship_in_Inventions_involving_AI_Activity_en.pdf)

-[A Study on Inventorship in Inventions Involving AI Activity, Dr Noam Shemtov - presentation for the Patent Law Committee (PDF, 345 KB)] (http://documents.epo.org/projects/babylon/eponet.nsf/0/3918F57B010A3540C125841900280653/$File/AI_inventorship_presentation_en.pdf)

-[Legal aspects of patenting inventions involving artificial intelligence Summary of feedback by EPC contracting states (PDF, 345 KB)] (http://documents.epo.org/projects/babylon/eponet.nsf/0/3918F57B010A3540C125841900280653/$File/AI_inventorship_summary_of_answers_en.pdf)

In the words of the EPO:

>The current European legal framework is **_“suitable for addressing the inventorship . . . of inventions involving AI activity”_**, and machines, regardless of their intelligence, should be considered **_“tools”._** The EPO is _“aware of discussions” about AI and inventorship_, **_but reaffirmed its position that an inventor “can only be a person”._**

And of Dr. Noam Shemtov:

>It is unlikely AI would advance to such a stage that it required changes in the law for **_“at least half a century”_**. **_“If and when we move to an era of AI systems possessing comprehensive knowledge and cognitive computing abilities, we will have to reassess our relationship with technology.”_**


#### **Fact 6:** Dabus’s patents currently under review by USPTO, EPO & UKIPO. Applications were also made with the WIPO.

-Initial stage of UKIPO’s patenting process has been passed with no further objections.

-The EPO sticks to above mentioned position.

-The USPTO, EPO and UKIPO would not comment on pending applications.  

-Applications with the WIPO World Intellectual Property Office, based in Geneva, are being filed first week of August 2019 (Prof Abbott). No further comment at this stage.


#### **Fact 7:** Prof Abbott _called for a new legal regime **“fit for purpose”**_.

Prof Abbott envisioned that these suggested legal regime changes would be likely to _**constitute an incentive for innovation across the global AI sector.**_ This in our view is a major aspect to consider in the build-up of a robust AA(L)E-based economy. In Prof Abbot’s words:

> **“If you make a point of recognizing how valuable a machine has been in the creative process, that machine will inevitably become more valuable.”**


### Discussion

As pointed out by Dazza, if you put Fact 3 and Fact 5 in perspective …

**Fact 3:** as emphasized by Prof. Abbott and Dr. Thaler,   “What’s striking is that the machine invented in two very different areas, **_neither of which its programmer had any background in.”_**

#### **Fact 5:** In the words of European Patent Office position on AI as "inventors":
>The current European legal framework is **_“suitable for addressing the inventorship . . . of inventions involving AI activity”_**, and machines, regardless of their intelligence, should be considered **_“tools”._** The EPO is _“aware of discussions” about AI and inventorship_, **_but reaffirmed its position that an inventor “can only be a person”._**

… then you come to this inevitable conclusion- in Dazza’s words:

>When an object is inventing things for a user, (things) the user has no background in, (then) that object becomes much more than a controlled tool, and more like a phenomenon set in motion. Dare I say it operates in some ways like a “Legal Agent”?

Our (Chris) comment:

>“... it operates in some way like a legal agent” then next thing to come to mind could be, if we apply that Agency framework, how to we define the Scope of Authority of said legal agent- in the context of its autonomous behavior as a “phenomenon set in motion” indeed much more than a tool fully “controlled” by the hand of its user/master. That gives us pause but at the same time it sounds like a valid & legitimate working topic: not just another out-on-a-limb AI speculation, since it has, after all, just now really happened. It’s real folks.

One Computational Law chat colleague had a rather valid counterpoint (EPO), she said:
>Going back to the concept of patents, as a right to exploit a given invention, it would be a problem to define “to whom” (legal person). This goes to _**legal personality**_ (even if companies are an abstract construction, in very narrow terms they’re also considered with personality rights (as _“name and credit”_). I am not so familiar with US patent law but in the case of trademarks (my experience) _**there is the need to assert the intent to use**_ (this would link to the core, genesis of the patent, a monopoly right granted by the State.. ). We also always need to consider the morals rights (ethical questions). I would go with EPO argument, _since a machine has no legal personality,_ so that would be a right attributed to no one... The case for companies or partnerships would be easier to get over with the “natural persons” approach.

Our colleague had more considerations on agency & legal personality, to integrate into later editions.

We venture below some non-Lawyer thoughts, _having not yet read the full 3 EPO reports (soon...)._


So what do we have- an automated & autonomous entity (an AAE): indeed as a matter of fact rather than a legal opinion from a human standpoint, it is not sentient nor self-aware, it is not a natural person nor and individual (not an NPI)- therefore _**it can’t function within the paradigm of “intent” that first requires to be dealing with an NPI**_, to be then deployed in relevant bodies of Law (Criminal, Contract, Tort, Trademark as also mentioned, etc.). 

Which connects back to the concept of “crumple zone” whereas the NPI most closely linked to the AAE in the chain of events takes the blame for any consequences of the AAE’s actions. 

In current & future larger & more complex AAE set-ups, there is a risk that this AAE-closest-NPI link becomes tenuous or even may not be possibly defined. Thus, there’s a risk of a legal vacuum developing fast (and in much less time than 50 years!) that makes it difficult to apply legal analysis methods & frameworks (same for ethical i.o. legal frameworks). 

This might provide sufficient justification to ask the (Dazza) question whereas the AAE operates in same ways like a “Legal Agent”. Now if you would affix “Legal Agent” traits to this AAE, legal analysis could be performed starting with where does the AAE sits on the R4 mapping (Roles & Relationships, Rights and Responsibilities), etc.  

To take it one step further…

This designer-AAE may be connected to a means of production (think deep-Space 3D-printing factory, whatever) that produces the stuff. Then the stuff itself goes into AAE mode, and does things that have some impact over NPI living in its vicinity- which leads us to a moral / ethical crumple zone concept: the nearest NPI in the chain of events & consequences takes the blame. 

But, that also connects back to the problematics of “Code is Free Speech” and “Code that exists sitting somewhere on a hardware means of electronic substrate is one thing, Code that is executing and doing the function that may have harmful consequences is another thing” (Brenden). (And which we could expand to “how about Code connected to a means of bionic or bio-neurocybernetic substrate linked to an extended NPI?”). 

In the patent example, the link to the closest NPIs in the chain of events (professor & his crew) is easy to establish, but it could become much more tenuous in larger contexts of AAEs.  So it could be logically argued that we run the risk of allowing for development of legal & ethical vacuums if we (lazily) bail ourselves out of the crumple zone by using the excuse of this increasingly tenuous connection to the nearest NPI.

In other words, the possibility that such a connection exists and can still be seen as sufficient (to act as foundation to the legal & ethical analysis frameworks to be used), this assumed fact still may not exonerate ourselves from considering the relevance & utility to apply “Legal Agent” traits to this AAE even if it’s not an NPI itself. 

This (lazy) self-exoneration might feel even more wrong as AAE develop cognitive & emotional communication protocols that make them look & feel much closer to an NPI (in a Stanley Kubrick sort of ways), but there would still be ground to deny them Free Will and therefore “intent” and maintain them into status of “AAE-tethered-to-NPI” to rebound on Brenden’s characterization.  

_But then we develop that sort of paradox whereas, on one hand, **the notion of “ad hoc control of AAE by NPI” becomes increasingly irrelevant**, and on the other hand, **we still want to treat these AAE as legally & ethically tethered to their remote creator & users NPI.**_  

We could ask whether this is analogous to like a kind of “Hillel ethical questioning” on the nature of AAEs and how they should proceed in the way they interact with their ecosystems.

But even if we think we don’t need to go that far yet, and we even don’t know how long we can entertain & indulge in that borderline complacent attitude (in any case we doubt it would take as long as 50 years), the mere fact that we have this vacuum developing fast, simply doesn’t exonerate ourselves from at least considering “Legal Agent” traits for this AAE. 

It’s possible that we need some kind of reality-tsunami, brutally unleashed strings of unintended consequences, to start realizing that we can’t anymore deal with legal & ethical consequences of the actions of these AAEs by merely relying on the crumple zone’s tenuous connection to the nearest NPI. 

In any case, at some point, a good lawyer might be able to get the judge to exonerate à la Pontius Pilate even the closest NPI involved, and then we start washing our hands above some serious legal & ethical vacuum. 

So, if this is sufficient to argue in favor of streamlining this vacuum, by outfitting this vacuum with a legal framework, then who/what else than this non-NPI AAE should be affixed with the trait of a “Legal Agent” and its characteristics? 

Then anyone involved may start pounding on this really nice working topic, and first we’d need to sort out the R4- Roles & Relationships, Rights & Responsibilities, for each party involved: where on the R4 mapping does an AAE sit, that is also a “Legal Agent” etc. 

This isn’t as weird and “fringe” legal analysis as it sounds. Or maybe it is, and we (Chris) could be totally wrong, but methink it’s a no-brainer that AAEs may eventually be given some trait of “Legal Agent”, then we can see how our current legal & ethical frameworks pan out across existing jurisdictions. Or, alternatively, such AAEs would have to be in some way re-attached to an AALE, and that AALE itself would be tackled either in the context of an existing Earth-based jurisdiction, or in any new legal localization, construct, and patterns remaining to be invented (think Space-based paradigms, more on this in later editions).

An interesting sub-plot could be whether and how could and should such AAE-Legal-Agents be then permitted and regulated to practice NPI’s Law & ethical regulatory mechanisms among themselves …

… doing this at the kind of breadth, scope and speed scale warranted by AAE intrinsic nature, and that an NPI wouldn’t be able to cope and deal with. Unless that NPI is extended (enhanced, augmented) by an AAE properly engineered to these legal & ethical effects, in which case what are precisely the engineering specs for these effects. 

Next sub-plot, whether and how should and could new legal & ethical frameworks be designed by AAE to suit themselves (perhaps with some guidance from NPI in the early stages) and at the same time work in harmonious ways that are compatible with and supporting of NPI’s own (human) legal and ethical frameworks …

… and then, in order to achieve that, _**one obligated point of passage might be to sort out how the various bodies of (human) Law figure and converge in a B/L/T/ construct.**_

And possibly also include ethical regulatory frameworks as a sort of (set of) additional functions on top of that B/L/T/ construct, in ways that are flexible, adaptable, evolutive, and most importantly, system- and context- dependent, rather than arbitrarily super-imposed. 

In Brenden’s words:
>At some point we will come to understand how the bodies of Law meet {computationally}

#
#
#

## Issue of control of AAE by NPI & transfer of control of AAE to NPI

We have had these discussions with Brenden a number of times, about the notions of _“controlling”_ or even _“intervening in the execution process”_ of AAEs for example for legal and/or ethical purposes. This latest exchange was referring to this particular recent piece from a certain well-known MSM outlet [Tech Tent: Robot rules for any Tom, Dick or Harry] (https://www.bbc.co.uk/news/technology-49204682) but remarks apply to a larger context.

In Brenden’s words, that we (Chris) totally second:

>There is a problem with the simplistic view of rules for machines presented in the article.

>Certainly if we relied on the machine to be tethered to a human as the author suggests- we would have crashed the lunar module on the Moon’s surface.

>It is quite silly to suggest that a human be in control of all sorts of systems. Robotic, automated and autonomous systems, dynamic systems, often deal with a level of complexity beyond human capability. If you’re sending a rocket to the Space you’re not going to rely on a human to decide when to separate the rocket modules.

>Complex dynamic systems and especially ones that involve time, complex sensor systems and multiple decisions- are not suited to have rules that simply say we should be able to turn over control to humans at any point. This is in fact it dangerous.

>The issue here is transfer of control to humans. But this is something that is system design and context dependent.

>Stating that we should have rules (statutes) around transfer control to humans- not only sets a dangerous precedent but legally compromises the millions (perhaps billions) of automated and autonomous systems currently operational.

>Transfer of control decisions can only be made by those who designed such systems- as these decisions are system dependent.

Further debunking that nonsense with a metaphor used earlier, this would be equivalent to peering through the glass window at the door of a medical operation block, constantly telling the surgeon how (s)he should operate, at intervals of infinitesimal fractions of a second.
